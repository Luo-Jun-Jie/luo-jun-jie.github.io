<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Junjie Luo - Academic Profile</title>
    <link rel="stylesheet" href="./src/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>

<body>

    <header>
        <div class="header-container">
            <!-- You can replace this with your logo if needed -->
            <!-- <img src="./img/logo.png" alt="Logo" class="header-logo" /> -->
            <h1>Junjie Luo</h1>
            <p>PhD Student, Electrical and Computer Engineering, Purdue University</p>
        </div>
    </header>

    <!-- Navigation with icons -->
    <nav>
        <button class="menu-item active" onclick="showSection('about')">
            <i class="fas fa-user"></i> About Me
        </button>
        <button class="menu-item" onclick="showSection('research')">
            <i class="fas fa-flask"></i> Research
        </button>
        <button class="menu-item" onclick="showSection('publications')">
            <i class="fas fa-book"></i> Publications
        </button>
        <button class="menu-item" onclick="showSection('contact')">
            <i class="fas fa-envelope"></i> Contact
        </button>
    </nav>

    <!-- Section content -->
    <main>
        <section id="about" class="section-content">
            <h2>About Me</h2>
            <p>Hello! I am Junjie Luo, a PhD student in Electrical and Computer Engineering at Purdue University. My
                research focuses on Depth from Defocus methods, particularly Depth from Couple Optical Differentiation
                (COD), and real-time depth estimation. I am passionate about advancing imaging techniques and developing
                computational methods to solve complex optical problems.</p>

            <!-- Added Short CV -->
            <div class="cv-section">
                <h3>Short CV</h3>
                <ul>
                    <li>PhD Student, Electrical and Computer Engineering, Purdue University (Present)</li>
                    <li>MSc in Computer Information and Technology, Purdue University</li>
                    <li>BSc in Computer Science, Sun Yat-sun University</li>
                </ul>
            </div>
        </section>

        <section id="research" class="section-content hidden">
            <h2>Research</h2>
            <p>My research interests include:</p>
            <ul>
                <li>Depth from Defocus (DfD) techniques</li>
                <li>Computational imaging</li>
                <li>Real-time depth estimation</li>
                <li>Optical differentiation methods</li>
            </ul>
            <p>Currently, I am working on a method called Depth from Couple Optical Differentiation, which involves
                capturing multiple images with varying aperture sizes and optical power to estimate object depth in
                real-time.</p>
        </section>

        <section id="publications" class="section-content hidden">
            <h2>Publications</h2>
            <div class="publications-container">
                <div class="publication-card">
                    <div class="publication-info">
                        <h3>Depth from Coupled Optical Differentiation</h3>
                        <p><strong>Authors:</strong> Luo, J., Liu, Y., Alexander, E. and Guo, Q.</p>
                        <p class="publication-abstract"><strong>Abstract:</strong>
                            <br>We propose depth from coupled optical differentiation, a low-computation passive-lighting 3D sensing mechanism. It is based on our discovery that per-pixel object distance can be rigorously determined by a coupled pair of optical derivatives of a defocused image using a simple, closed-form relationship. Unlike previous depth-from-defocus (DfD) methods that leverage spatial derivatives of the image to estimate scene depths, the proposed mechanismâ€™s use of only optical derivatives makes it significantly more robust to noise. Furthermore, unlike many previous DfD algorithms with requirements on aperture code, this relationship is proved to be universal to a broad range of aperture codes.
                            <br>We build the first 3D sensor based on depth from coupled optical differentiation. Its optical assembly includes a deformable lens and a motorized iris, which enables dynamic adjustments to the optical power and aperture radius. The sensor captures two pairs of images: one pair with a differential change of optical power and the other with a differential change of aperture scale. From the four images, a depth and confidence map can be generated with only 36 floating point operations per output pixel (FLOPOP), more than ten times lower than the previous lowest passive-lighting depth sensing solution to our knowledge. Additionally, the depth map generated by the proposed sensor demonstrates more than twice the working range of previous DfD methods while using significantly lower computation.
                        </p> <!-- Abstract placeholder -->
                    </div>
                    <div class="publication-meta">
                        <p><strong>Published:</strong> 2024, arXiv preprint arXiv:2409.10725</p>
                    </div>
                </div>
                <div class="publication-card">
                    <div class="publication-info">
                        <h3>CT-Bound: Fast Boundary Estimation From Noisy Images</h3>
                        <p><strong>Authors:</strong> Xu, W., Luo, J. and Guo, Q.</p>
                        <p class="publication-abstract"><strong>Abstract:</strong> 
                            <br>We present CT-Bound, a robust and fast boundary detection method for very noisy images using a hybrid Convolution and Transformer neural network. The proposed architecture decomposes boundary estimation into two tasks: local detection and global regularization. During the local detection, the model uses a convolutional architecture to predict the boundary structure of each image patch in the form of a pre-defined local boundary representation, the field-of-junctions (FoJ). Then, it uses a feed-forward transformer architecture to globally refine the boundary structures of each patch to generate an edge map and a smoothed color map simultaneously. Our quantitative analysis shows that CT-Bound outperforms the previous best algorithms in edge detection on very noisy images. It also increases the edge detection accuracy of FoJ-based methods while having a 3-time speed improvement. Finally, we demonstrate that CT-Bound can produce boundary and color maps on real captured images without extra fine-tuning and real-time boundary map and color map videos at ten frames per second.
                        </p> <!-- Abstract placeholder -->
                    </div>
                    <div class="publication-meta">
                        <p><strong>Published:</strong> 2024, arXiv preprint arXiv:2403.16494</p>
                    </div>
                </div>
                <div class="publication-card">
                    <div class="publication-info">
                        <h3>Generative Quanta Color Imaging</h3>
                        <p><strong>Authors:</strong> Purohit, V., Luo, J., Chi, Y., Guo, Q., Chan, S.H. and Qiu, Q.</p>
                        <p class="publication-abstract"><strong>Abstract:</strong> 
                            <br>The astonishing development of single-photon cameras has created an unprecedented opportunity for scientific and industrial imaging. However, the high data throughput generated by these 1-bit sensors creates a significant bottleneck for low-power applications. In this paper, we explore the possibility of generating a color image from a single binary frame of a single-photon camera. We evidently find this problem being particularly difficult to standard colorization approaches due to the substantial degree of exposure variation. The core innovation of our paper is an exposure synthesis model framed under a neural ordinary differential equation (Neural ODE) that allows us to generate a continuum of exposures from a single observation. This innovation ensures consistent exposure in binary images that colorizers take on, resulting in notably enhanced colorization. We demonstrate applications of the method in single-image and burst colorization and show superior generative performance over baselines. Project website can be found at this https URL.
                        </p> <!-- Abstract placeholder -->
                    </div>
                    <div class="publication-meta">
                        <p><strong>Published:</strong> 2024, IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
                    </div>
                </div>
            </div>
        </section>
        
        

        <section id="contact" class="section-content hidden">
            <h2>Contact</h2>
            <p>You can reach me at:</p>
            <ul>
                <li>Email: luo330@purdue.edu</li>
                <li>Office: Purdue University, Electrical and Computer Engineering Department</li>
            </ul>
        </section>
    </main>

    <footer>
        <div class="footer-content">
            <div class="social-icons">
                <a href="https://www.linkedin.com/in/junjiel/" target="_blank"><i class="fab fa-linkedin"></i></a>
                <a href="https://github.com/guo-research-group" target="_blank"><i class="fab fa-github"></i></a>
                <a href="mailto:luo330@purdue.edu"><i class="fas fa-envelope"></i></a>
            </div>
            <img src="./img/Purdue_logo.png" alt="Purdue University Logo" class="purdue-logo">
        </div>
        <div class="footer-bottom">
            <p>&copy; 2024 Junjie Luo | Last updated: October 2024</p>
        </div>
    </footer>

    <script src="./src/scripts.js"></script>

</body>

</html>